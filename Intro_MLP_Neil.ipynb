{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neilus03/DEEP-LEARNING-2023/blob/main/Intro_MLP_Neil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zLU1oYNLpv2"
      },
      "source": [
        "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/dkaratzas/DL2022-23/blob/main/Problems%203%20-%20Intro%20MLPs/P3_Intro_MLP.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOMdMgG8h88o"
      },
      "source": [
        "# Simple MLP in PyTorch\n",
        "\n",
        "In this notebook we will detail how to create and train a multilayer perceptron using pytorch. We will go through:\n",
        " \n",
        "1. Two different ways of creating an MLP\n",
        "2. Create a standard training loop\n",
        "3. Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIr2ty0tFA4C"
      },
      "outputs": [],
      "source": [
        "import torch #should be installed by default in any colab notebook\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWLIxo9Oigfo"
      },
      "outputs": [],
      "source": [
        "# If this cell fails you need to change the runtime of your colab notebook to GPU\n",
        "# Go to Runtime -> Change Runtime Type and select GPU\n",
        "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
        "\n",
        "# use gpu if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI_YXyigdTUC"
      },
      "source": [
        "# Data\n",
        "\n",
        "Before training we need data! So lets create an artificial dataset for our model to learn. This is just a setup section, so you can skip it and move on to the model definition if you want.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j02Wymqldd76"
      },
      "outputs": [],
      "source": [
        "#@title The code in this cell defines a function to generate training and validation data, no need to worry about it. Just hit the play button before continuing { display-mode: \"form\" }\n",
        "def generate_data(n_samples, generator, regression=False, noise_scale=2, **kwargs):\n",
        "\n",
        "    if \"noise\" in kwargs:\n",
        "        kwargs[\"noise\"] *= noise_scale\n",
        "    \n",
        "    x_train, y_train = generator(n_samples, **kwargs) # training data\n",
        "\n",
        "    x_val, y_val = generator(n_samples, **kwargs) # validation data\n",
        "\n",
        "    # Plot the data\n",
        "    fig, ax = plt.subplots(1, 2)\n",
        "    ax[0].set_title(\"Training Data\") \n",
        "    ax[1].set_title(\"Validation Data\")\n",
        "\n",
        "    if regression:\n",
        "        ax[0].scatter(x_train, y_train, cmap=plt.cm.coolwarm)\n",
        "        ax[1].scatter(x_val, y_val, cmap=plt.cm.coolwarm)\n",
        "    else:\n",
        "        ax[0].scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap=plt.cm.coolwarm)\n",
        "        ax[1].scatter(x_val[:, 0], x_val[:, 1], c=y_val, cmap=plt.cm.coolwarm)\n",
        "\n",
        "    return x_train, y_train, x_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxKIZ84cLpv9"
      },
      "outputs": [],
      "source": [
        "# find more datasets at https://scikit-learn.org/stable/modules/classes.html#samples-generator\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "N = 1000 # number of sample\n",
        "xBlobsTrain, yBlobsTrain, xBlobsVal, yBlobsVal = generate_data([N, N], make_blobs, centers=[[0, 0.5], [0, -0.5]], cluster_std=0.2)\n",
        "\n",
        "#Convert the data from numpy arrays into PyTorch tensors\n",
        "xBlobsTrain = torch.from_numpy(xBlobsTrain).float()\n",
        "yBlobsTrain = torch.from_numpy(yBlobsTrain).long()\n",
        "xBlobsVal = torch.from_numpy(xBlobsVal).float()\n",
        "yBlobsVal = torch.from_numpy(yBlobsVal).long()\n",
        "\n",
        "# move data to gpu if available\n",
        "xBlobsTrain = xBlobsTrain.to(device)\n",
        "yBlobsTrain = yBlobsTrain.to(device)\n",
        "\n",
        "xBlobsVal = xBlobsVal.to(device)\n",
        "yBlobsVal = yBlobsVal.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eia08EI34Dug"
      },
      "outputs": [],
      "source": [
        "#@title The code in this cell is just for visualization no need to worry about it. Just hit the play button before continuing { display-mode: \"form\" }\n",
        "# Dont worry about the code in this cell it is just for visualization purposes you dont need to understand or edit it yet\n",
        "\n",
        "def plot_model(x, y, model, axis):\n",
        "\n",
        "    mesh = torch.arange(-2, 2, 0.01)\n",
        "\n",
        "    xx, yy = torch.meshgrid(mesh, mesh)\n",
        "    with torch.no_grad():\n",
        "        data = torch.from_numpy(np.vstack((xx.reshape(-1), yy.reshape(-1))).T).float()\n",
        "        Z = model(data.cuda().detach())\n",
        "    Z = Z.max(1)[1].reshape(xx.shape)\n",
        "    axis.contourf(xx, yy, Z.cpu(), cmap=plt.cm.coolwarm, alpha=0.3)\n",
        "    axis.scatter(x[:, 0].cpu(), x[:, 1].cpu(), c=y.cpu(), s=20, cmap=plt.cm.coolwarm)\n",
        "    axis.get_xaxis().set_ticks([])\n",
        "    axis.get_yaxis().set_ticks([])\n",
        "\n",
        "\n",
        "def plot_regressor(x, y, model, axis):\n",
        "    \n",
        "    x_min = torch.min(x.detach())\n",
        "    x_max = torch.max(x.detach())\n",
        "    x_range = torch.linspace(x_min, x_max, 100).unsqueeze(1).to(device)\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        y_pred = model(x_range).detach()\n",
        "        \n",
        "    axis.scatter(x.cpu(), y.cpu())\n",
        "    axis.plot(x_range.cpu(), y_pred.cpu(), 'r-', lw=5, label=\"Model Prediction\")\n",
        "    axis.get_xaxis().set_ticks([])\n",
        "    axis.get_yaxis().set_ticks([])\n",
        "    \n",
        "def draw_plots(x, y, model, losses, visualize_surface = False, visualize_regressor = False):\n",
        "\n",
        "    if visualize_surface and visualize_regressor:\n",
        "        raise ValueError(\"Expected only one of 'visualize_error' or 'visualize_regressor' to be True.\")\n",
        "\n",
        "    if visualize_surface:\n",
        "        fig, ax = plt.subplots(1, 2)\n",
        "        ax[0].set_title(\"Output Space\")\n",
        "        ax[1].set_title(\"Losses\")\n",
        "        ax[1].plot(losses[\"train\"], label=\"training loss\")\n",
        "        ax[1].plot(losses[\"val\"], label=\"validation loss\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        plot_model(x, y, model, ax[0])\n",
        "    \n",
        "    elif visualize_regressor:\n",
        "        fig, ax = plt.subplots(1, 2)\n",
        "        ax[0].set_title(\"Validation Data\")\n",
        "        ax[1].set_title(\"Losses\")\n",
        "        ax[1].plot(losses[\"train\"], label=\"training loss\")\n",
        "        ax[1].plot(losses[\"val\"], label=\"validation loss\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        plot_regressor(x, y, model, ax[0])\n",
        "\n",
        "    else:\n",
        "        plt.plot(losses[\"train\"], label=\"training loss\")\n",
        "        plt.plot(losses[\"val\"], label=\"validation loss\")\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.pause(0.000001)\n",
        "    plt.show()   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08gasNsyLpv_"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlFN480ikiJq"
      },
      "source": [
        "Now that these basic concepts are out of the way and we have our data lets take a look at how to create a simple MLP in PyTorch\n",
        "\n",
        "The ```torch.nn``` [package](https://pytorch.org/docs/stable/nn.html) is the one containing all of the neural network related layers, operations etc.\n",
        "\n",
        "In order to create a simple MLP we can define a sequential module like so:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(nn.Linear(inp_dim, hidden_dim),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_dim, output_dim))\n",
        "```\n",
        "\n",
        "So now we can feed data into this model and the ```nn``` module will take care of the forward pass.\n",
        "\n",
        "Some more complicated acrchitectures will require you to detail the forward pass of the model. This will require creating a class that inherits from ```nn.Module``` and define its ```forward``` and ```__init__``` methods.\n",
        "\n",
        "```\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(inp_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        return self.fc2(out)\n",
        "```\n",
        "\n",
        "Notice how this approach allows more flexibility since we can insert ANY python statement in the forward method, such as ```print```, ```if else```, etc.\n",
        "\n",
        "Since our model for this first session is very simple we will use the first approach in this notebook, but we encourage you to try your own model using the second approach.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyEjLgfjrWzs"
      },
      "source": [
        "## Linear Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viAIdmoKLpwA"
      },
      "source": [
        "We will start by defining a very simple, linear model. This is a very inefficient model, but it serves us in order to have a template to build on afterwards.\n",
        "\n",
        "The input of this model will be set to 2 (we have two features in our datasets) and the output to 2 (we will predict one of two classes).\n",
        "\n",
        "The output of the model will be a raw, unnormalized score for each class. The scores (the logits) that will then go through softmax plus cross entropy loss to compare against the true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAoS_kiRZPYT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn # nn package to create our linear model\n",
        "\n",
        "D = 2  # input dimensions\n",
        "C = 2  # num_classes\n",
        "H = 100  # num_hidden_units\n",
        "\n",
        "# each Linear module has a weight and bias\n",
        "modelMLP = nn.Sequential(\n",
        "    nn.Linear(D, H),\n",
        "    nn.Linear(H, C)\n",
        ")\n",
        "\n",
        "# move model to gpu if available\n",
        "modelMLP.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2vREUPdBG6w"
      },
      "source": [
        "---\n",
        "\n",
        "<font color=\"blue\">**Exercise 1**: Redefine the model by implementing a Class that inherits from `nn.Module()` and implementing its `__init__()` and `forward()` methods.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpX5QglwLpwB"
      },
      "outputs": [],
      "source": [
        "# Your Code Here\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, inp_dim = 2, hidden_dim = 100, output_dim = 2):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(inp_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = SimpleMLP(hidden_dim=50)\n",
        "a(torch.tensor([float(5), float(2)]))"
      ],
      "metadata": {
        "id": "9KHq_de1L5o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "526cprEaLpwB"
      },
      "source": [
        "## The Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-Fb-PmLpwB"
      },
      "source": [
        "We will use a criterion (cost) that combines the calculation of softmax and a cross entropy loss in a single block, which is numerically more stable than doing these two steps separately:\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YWPIa6FLpwB"
      },
      "outputs": [],
      "source": [
        "# we use softmax + cross entropy loss (combined in a single function which unfortunately is called just CrossEntropyLoss) for our classification task\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3FGH06zLpwB"
      },
      "source": [
        "The `torch.nn` package has many more loss functions defined. You can check them out here: https://pytorch.org/docs/stable/nn.html#loss-functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMeMyAHVw8yp"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bd83YOhLpwB"
      },
      "source": [
        "Once we have a model defined, we should set up our training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxawlAI3LpwC"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0) # seed for reproductibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUaUnQJULpwC"
      },
      "source": [
        "## The optimiser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49xxs7vqLpwC"
      },
      "source": [
        "The `optimiser` that we define is an object that will take care the update of all the weights in our model. For this, it needs to know where the weights are, and what update rule (optimisation algorithm) to use.\n",
        "\n",
        "For now we will stick with the \"Stochastic Gradient Descent\" optimiser. We define this next, setting up also the learning rate, and a few hyperparameters (weight decay and momentum) that we will discuss in more detail in the next lecture.\n",
        "\n",
        "In reality, this optimiser is just doing standard gradient descent - so here's another misnomer. Whether we will use it in a \"stochastic\" setting or not, depends on us - on how we will feed in the data during the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYJU2MNgLpwC"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "lambda_l2 = 1e-5\n",
        "\n",
        "# we use the optim package to apply gradient descent for our parameter updates\n",
        "optimizer = torch.optim.SGD(modelMLP.parameters(), lr=learning_rate, momentum=0.9, weight_decay=lambda_l2) # built-in L2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqjyLuFvLpwD"
      },
      "source": [
        "## The training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJSG9uWTLpwD"
      },
      "source": [
        "Next we will define our training loop: feedforward, calculate loss, back propagate, and update the weights.\n",
        "\n",
        "In every step we also keep track of the evolution of our loss, and also the accuracy of our classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n_iIYfRtUiz"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train(x_train, y_train, x_val, y_val, criterion, model, optimizer, epochs=300, visualize_surface=False):\n",
        "    \n",
        "    losses = {\"train\": [], \"val\": []} # Two lists to keep track of the evolution of our losses\n",
        "\n",
        "    for t in range(epochs):\n",
        "        \n",
        "        # activate training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Feed forward to get the logits\n",
        "        y_pred = model(x_train) # x_train is the whole batch, so we are doing full batch gradient descent here\n",
        "    \n",
        "        # Compute the loss\n",
        "        loss = criterion(y_pred.squeeze(), y_train)\n",
        "        \n",
        "        # zero the gradients before running the backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Backward pass to compute the gradient of loss w.r.t our learnable params\n",
        "        loss.backward()\n",
        "\n",
        "        # Update params\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval() # we don't need gradients on to calculate performance, just simple inference\n",
        "        \n",
        "        # Compute the accuracy.\n",
        "        score, predicted = torch.max(y_pred, dim=1) # torch.max() returns the maximum value and the argmax (index of the maximum value)\n",
        "        train_acc = (y_train == predicted).sum().float() / len(y_train)\n",
        "        losses[\"train\"].append(loss.item() ) # keep track of our training loss\n",
        "\n",
        "        # Run model on validation data\n",
        "        val_loss, val_acc = calculateLossAcc(criterion, model, x_val, y_val) # Call our helper function (see below) on the validation set\n",
        "        losses[\"val\"].append(val_loss.item()) # keep track of our validation loss\n",
        "\n",
        "        # Create plots\n",
        "        display.clear_output(wait=True)\n",
        "        draw_plots(x_val, y_val, model, losses, visualize_surface, visualize_regressor = False)\n",
        "\n",
        "        print(\"Training: [EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), train_acc))        \n",
        "        print(\"Validation: [EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, val_loss.item(), val_acc))\n",
        "        \n",
        "    return losses # In case we want to plot them afterwards\n",
        "\n",
        "# A helper function that calculates our loss and accuracy on a given dataset (by default on our validation set)\n",
        "def calculateLossAcc(criterion, model, x, y):\n",
        "\n",
        "    # set model in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad(): # do not compute gradients for validation\n",
        "        y_pred = model(x)\n",
        "\n",
        "\n",
        "    # compute loss and accuracy \n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "    loss = criterion(y_pred.squeeze(), y)\n",
        "    acc = (y == predicted).sum().float() / len(y)\n",
        "\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Prc0qFO0SR5"
      },
      "source": [
        "## Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psBKsIFAtkXQ"
      },
      "outputs": [],
      "source": [
        "# You can set visualize_surface to False to omit the visualization of the output space. The training will be much faster this way\n",
        "\n",
        "losses = train(xBlobsTrain, yBlobsTrain, xBlobsVal, yBlobsVal, criterion, modelMLP, optimizer, visualize_surface=True, epochs = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rNTI75LLpwE"
      },
      "outputs": [],
      "source": [
        "# Draw plots once at the end\n",
        "\n",
        "draw_plots(xBlobsVal, yBlobsVal, modelMLP, losses, visualize_surface = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1uzixtfLpwE"
      },
      "source": [
        "---\n",
        "\n",
        "<font color=\"blue\">**Exercise 2**: Redefine the model, but now add a non-linear activation function at the hidden layer. Check the `torch.nn.functional` module for possible activation functions to use. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty-AmidwLpwE"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "D = 2  # input dimensions\n",
        "C = 2  # num_classes\n",
        "H = 100  # num_hidden_units\n",
        "\n",
        "# define the model with a non-linear activation function\n",
        "modelMLP2 = nn.Sequential(\n",
        "    nn.Linear(D, H),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(H, C)\n",
        ")\n",
        "\n",
        "modelMLP2.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Let´s test it, just display it if you want to see the code. If you just want to check result and conclusions, running it is enugh.\n",
        "learning_rate = 1e-2\n",
        "lambda_l2 = 1e-5\n",
        "\n",
        "# we use the optim package to apply gradient descent for our parameter updates\n",
        "optimizer = torch.optim.SGD(modelMLP2.parameters(), lr=learning_rate, momentum=0.9, weight_decay=lambda_l2) # built-in L2\n",
        "\n",
        "# You can set visualize_surface to False to omit the visualization of the output space. The training will be much faster this way\n",
        "losses = train(xBlobsTrain, yBlobsTrain, xBlobsVal, yBlobsVal, criterion, modelMLP2, optimizer, visualize_surface=True, epochs = 100)"
      ],
      "metadata": {
        "id": "YdSrahV3APun",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried several learning rates and found out that for this particular case 0.01 of learning rate drags my loss down much faster and more than the previous modelMLP without ReLU activation function and with a lower learning rate.\n",
        "\n",
        "This gain in loss (a lower cost) is because ReLU is a non-linear activation function that introduces non-linearity into the neural network, allowing it to learn more complex and non-linear decision boundaries."
      ],
      "metadata": {
        "id": "2B-BJXkyCLIS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gABBONKvLpwE"
      },
      "source": [
        "## Clean up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbEF11KvLpwE"
      },
      "source": [
        "Example code to delete data and clear the cuda cache. Do not run it if you plan to go back to the cells above to try more things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq9tF9i9LpwE"
      },
      "outputs": [],
      "source": [
        "del xBlobsTrain, yBlobsTrain, xBlobsVal, yBlobsVal\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpZG0az2slrJ"
      },
      "source": [
        "# Lets try with a more complex dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4AD7XPKLpwF"
      },
      "outputs": [],
      "source": [
        "def make_spiral(n_points, noise=0.5):\n",
        "\n",
        "    n = np.sqrt(np.random.rand(n_points, 1)) * 780 * (2*np.pi)/360\n",
        "    d1x = -np.cos(n)*n + np.random.rand(n_points, 1) * noise\n",
        "    d1y = np.sin(n)*n + np.random.rand(n_points, 1) * noise\n",
        "\n",
        "    x, y = (np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x + 1, -d1y)))),\n",
        "            np.hstack((np.zeros(n_points), np.ones(n_points))).astype(\"int\"))\n",
        "\n",
        "    x = (x - x.mean()) / x.std()\n",
        "\n",
        "    return x, y\n",
        "\n",
        "N = 1000 # number of samples\n",
        "xSpiralTrain, ySpiralTrain, xSpiralVal, ySpiralVal = generate_data(N, make_spiral, noise=0.5)\n",
        "\n",
        "#Convert the data from numpy arrays into PyTorch tensors\n",
        "xSpiralTrain = torch.from_numpy(xSpiralTrain).float()\n",
        "ySpiralTrain = torch.from_numpy(ySpiralTrain).long()\n",
        "xSpiralVal = torch.from_numpy(xSpiralVal).float()\n",
        "ySpiralVal = torch.from_numpy(ySpiralVal).long()\n",
        "\n",
        "# move data to gpu if available\n",
        "xSpiralTrain = xSpiralTrain.to(device)\n",
        "ySpiralTrain = ySpiralTrain.to(device)\n",
        "\n",
        "xSpiralVal = xSpiralVal.to(device)\n",
        "ySpiralVal = ySpiralVal.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MVjaTWN0G00"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NyqlO9NLpwF"
      },
      "outputs": [],
      "source": [
        "# You can set visualize_surface to False to omit the visualization of the output space. The training will be much faster this way\n",
        "\n",
        "losses = train(xSpiralTrain, ySpiralTrain, xSpiralVal, ySpiralVal, criterion, modelMLP, optimizer, visualize_surface=True, epochs = 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5MDnOZ5LpwF"
      },
      "outputs": [],
      "source": [
        "# Draw plots once at the end\n",
        "\n",
        "draw_plots(xSpiralVal, ySpiralVal, modelMLP, losses, visualize_surface = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guTXtSlULpwG"
      },
      "source": [
        "## Trying more complex models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LqbLBTOLpwG"
      },
      "source": [
        "As expected, the model we created is too simple to learn anything useful. It can only learn a linear decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2aA8DTFLpwG"
      },
      "source": [
        "<font color=\"blue\">**Exercise 3:** Can you create a model that performs well on this classification task? Change the depth of the model, the number of neurons in different layers, and the activation functions. You might also need to adjust your learning rate and/or the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjLdZMviLpwH"
      },
      "outputs": [],
      "source": [
        "# Your Code Here\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class MoreComplexMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, inDim = 2, outDim = 2, hidden_dim = 100 ):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(inDim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc5 = nn.Linear(hidden_dim, outDim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.fc4(out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.fc5(out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        return out\n",
        "    \n",
        "modelComplex = MoreComplexMLP(D, C, H)\n",
        "\n",
        "# move model to gpu if available\n",
        "modelComplex.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-2\n",
        "lambda_l2 = 1e-5\n",
        "\n",
        "# we use the optim package to apply gradient descent for our parameter updates\n",
        "optimizer = torch.optim.SGD(modelComplex.parameters(), lr=learning_rate, momentum=0.9, weight_decay=lambda_l2) # built-in L2"
      ],
      "metadata": {
        "id": "kOK59kDUatrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = train(xSpiralTrain, ySpiralTrain, xSpiralVal, ySpiralVal, criterion, modelComplex, optimizer, visualize_surface=False, epochs = 1000)"
      ],
      "metadata": {
        "id": "xyoJPAEIad7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_plots(xSpiralVal, ySpiralVal, modelComplex, losses, visualize_surface = True)"
      ],
      "metadata": {
        "id": "Yb06ieAuanwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMw29uaULpwH"
      },
      "source": [
        "## Clean up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHmnuHAALpwH"
      },
      "source": [
        "Example code to delete data and clear the cuda cache. Do not run it if you plan to go back to the cells above to try more things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO5Al66MLpwI"
      },
      "outputs": [],
      "source": [
        "del xSpiralTrain, ySpiralTrain, xSpiralVal, ySpiralVal\n",
        "del modelMLP, modelComplex\n",
        "del criterion\n",
        "del optimizer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgRziMgUIegD"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAcZAkgZLpwI"
      },
      "source": [
        "We can use the same procedure to tackle a regression problem. In this case, we will be producing a continuous output value, and we need to use a different loss.\n",
        "\n",
        "The code below generates a dataset that can be used for training a regression network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWgDVJRYrsi2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "N = 1000 # number of samples\n",
        "xRegTrain, yRegTrain, xRegVal, yRegVal = generate_data(N, make_regression, regression=True, noise=5, n_features=1, random_state=52)\n",
        "\n",
        "#Convert the data from numpy arrays into PyTorch tensors\n",
        "xRegTrain = torch.from_numpy(xRegTrain).float()\n",
        "yRegTrain = torch.from_numpy(yRegTrain).float()\n",
        "xRegVal = torch.from_numpy(xRegVal).float()\n",
        "yRegVal = torch.from_numpy(yRegVal).float()\n",
        "\n",
        "# move data to gpu if available\n",
        "xRegTrain = xRegTrain.to(device)\n",
        "yRegTrain = yRegTrain.to(device)\n",
        "\n",
        "xRegVal = xRegVal.to(device)\n",
        "yRegVal = yRegVal.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9bSfy0KLpwI"
      },
      "source": [
        "<font color=\"blue\">**Exercise 4:** Using the data generated modify the training code in the cell below to solve a regression problem.</font>\n",
        "\n",
        "\n",
        "> Hints\n",
        ">* Note that the generated dataset only has one feature\n",
        ">* You can use the same training and validation functions with some slight modifications since it doesn't make sense to calculate accuracy in a regression problem\n",
        ">* You can find different training criteria (a.k.a loss functions) in PyTorch [docs](https://pytorch.org/docs/stable/nn.html#loss-functions)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code Here\n",
        "import torch.nn as nn # nn package to create our linear model\n",
        "\n",
        "D = 1  # input dimensions\n",
        "C = 1  # num_classes\n",
        "H = 100  # num_hidden_units\n",
        "\n",
        "# each Linear module has a weight and bias\n",
        "reg_modelMLP = nn.Sequential(\n",
        "    nn.Linear(D, H),\n",
        "    nn.Linear(H, C)\n",
        ")\n",
        "\n",
        "# move model to gpu if available\n",
        "reg_modelMLP.to(device)"
      ],
      "metadata": {
        "id": "QY-SGDegBhWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title alternatively I could use this way to create the NN.\n",
        "\"\"\"\n",
        "\n",
        "class MoreComplexMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, inDim = 1, outDim = 1, hidden_dim = 100 ):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(inDim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, outDim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "    \n",
        "reg_model2 = MoreComplexMLP()\n",
        "\n",
        "# move model to gpu if available\n",
        "reg_model2.to(device)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HIg4TZt5Kl2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.MSELoss() #The most used loss for regression problems\n",
        "learning_rate = 1e-3\n",
        "#lambda_l2 = 1e-5\n",
        "\n",
        "# we use the optim package to apply gradient descent for our parameter updates\n",
        "optimizer = torch.optim.SGD(reg_modelMLP.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "0VD2WCybJ2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(x_train, y_train, x_val, y_val, criterion, model, optimizer, epochs=300, visualize_surface=False):\n",
        "    \n",
        "    losses = {\"train\": [], \"val\": []} # Two lists to keep track of the evolution of our losses\n",
        "\n",
        "    for t in range(epochs):\n",
        "        \n",
        "        # activate training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Feed forward to get the logits\n",
        "        y_pred = model(x_train) # x_train is the whole batch, so we are doing full batch gradient descent here\n",
        "    \n",
        "        # Compute the loss\n",
        "        loss = criterion(y_pred.squeeze(), y_train)\n",
        "        \n",
        "        # zero the gradients before running the backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Backward pass to compute the gradient of loss w.r.t our learnable params\n",
        "        loss.backward()\n",
        "\n",
        "        # Update params\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval() # we don't need gradients on to calculate performance, just simple inference\n",
        "        \n",
        "        # Compute the accuracy.\n",
        "        score, predicted = torch.max(y_pred, dim=1) # torch.max() returns the maximum value and the argmax (index of the maximum value)\n",
        "        losses[\"train\"].append(loss.item() ) # keep track of our training loss\n",
        "\n",
        "        # Run model on validation data\n",
        "        val_loss = calculateLoss(criterion, model, x_val, y_val) # Call our helper function (see below) on the validation set\n",
        "        losses[\"val\"].append(val_loss.item()) # keep track of our validation loss\n",
        "\n",
        "        # Create plots\n",
        "        display.clear_output(wait=True)\n",
        "        draw_plots(x_val, y_val, model, losses, visualize_surface = False, visualize_regressor = True)\n",
        "\n",
        "    return losses # In case we want to plot them afterwards\n",
        "\n",
        "# A helper function that calculates our loss and accuracy on a given dataset (by default on our validation set)\n",
        "def calculateLoss(criterion, model, x, y):\n",
        "\n",
        "    # set model in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad(): # do not compute gradients for validation\n",
        "        y_pred = model(x)\n",
        "\n",
        "\n",
        "    # compute loss and accuracy \n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "    loss = criterion(y_pred.squeeze(), y)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ZWntkwh-mGlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = train(xRegTrain, yRegTrain, xRegVal, yRegVal, criterion, reg_modelMLP, optimizer, visualize_surface=True, epochs = 15)"
      ],
      "metadata": {
        "id": "CDm4JAgVlRFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see it's a nice regression models that fits the data very well."
      ],
      "metadata": {
        "id": "cdVYzmVSJQI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loss report (just run it)\n",
        "print(\"TRAIN LOSSES:\")\n",
        "print()\n",
        "for i in losses['train']:\n",
        "    print(i)\n",
        "print()\n",
        "print(\"VALIDATION LOSSES:\")\n",
        "print()\n",
        "for i in losses['val']:\n",
        "    print(i)\n",
        "print()\n",
        "print(\"FINAL LOSS:   on training set =\", str(losses['train'][-1]) +\", on validation set =\", losses['val'][-1] )"
      ],
      "metadata": {
        "id": "0HKgDdmFNt7s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et1rctL9LpwJ"
      },
      "source": [
        "## Clean up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rCb_Xf_LpwJ"
      },
      "source": [
        "Example code to delete data and clear the cuda cache. Do not run it if you plan to go back to the cells above to try more things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DaT2-vYLpwJ"
      },
      "outputs": [],
      "source": [
        "del xRegTrain, yRegTrain, xRegVal, yRegVal\n",
        "del reg_modelMLP, criterion, optimizer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7kNmRZwLpwJ"
      },
      "source": [
        "# More Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2g6BGrILpwJ"
      },
      "source": [
        "In the next cell we will create a dataset with more than 2 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgfwjg6FLpwJ"
      },
      "outputs": [],
      "source": [
        "# find more datasets at https://scikit-learn.org/stable/modules/classes.html#samples-generator\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "N = 1000 # number of samples\n",
        "xManyClassTrain, yManyClassTrain, xManyClassVal, yManyClassVal = generate_data([N, N, N], make_blobs, centers=[[0, 0.5], [0, -0.5], [-0.5, -0.5]], random_state=12, cluster_std=0.2)\n",
        "\n",
        "#Convert the data from numpy arrays into PyTorch tensors\n",
        "xManyClassTrain = torch.from_numpy(xManyClassTrain).float()\n",
        "yManyClassTrain = torch.from_numpy(yManyClassTrain).long()\n",
        "xManyClassVal = torch.from_numpy(xManyClassVal).float()\n",
        "yManyClassVal = torch.from_numpy(yManyClassVal).long()\n",
        "\n",
        "# move data to gpu if available\n",
        "xManyClassTrain = xManyClassTrain.to(device)\n",
        "yManyClassTrain = yManyClassTrain.to(device)\n",
        "\n",
        "xManyClassVal = xManyClassVal.to(device)\n",
        "yManyClassVal = yManyClassVal.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DcmahR9LpwK"
      },
      "source": [
        "<font color=\"blue\">**Exercise 5:** Using the data generated in the cell above, modify your classification model to tackle a non-binary classification problem.</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "D = 2  # input dimensions\n",
        "C = 3  # num_classes\n",
        "H = 100  # num_hidden_units\n",
        "\n",
        "# define the model with a non-linear activation function\n",
        "modelMLP3 = nn.Sequential(\n",
        "    nn.Linear(D, H),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(H, C)\n",
        ")\n",
        "\n",
        "modelMLP3.to(device)"
      ],
      "metadata": {
        "id": "b84CeVlhGRk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = optim.Adam(modelMLP3.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "eSM8e5uRGUMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(x_train, y_train, x_val, y_val, criterion, model, optimizer, epochs=300, visualize_surface=False):\n",
        "    \n",
        "    losses = {\"train\": [], \"val\": []} # Two lists to keep track of the evolution of our losses\n",
        "\n",
        "    for t in range(epochs):\n",
        "        \n",
        "        # activate training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Feed forward to get the logits\n",
        "        y_pred = model(x_train) # x_train is the whole batch, so we are doing full batch gradient descent here\n",
        "    \n",
        "        # Compute the loss\n",
        "        loss = criterion(y_pred.squeeze(), y_train)\n",
        "        \n",
        "        # zero the gradients before running the backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Backward pass to compute the gradient of loss w.r.t our learnable params\n",
        "        loss.backward()\n",
        "\n",
        "        # Update params\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval() # we don't need gradients on to calculate performance, just simple inference\n",
        "        \n",
        "        # Compute the accuracy.\n",
        "        score, predicted = torch.max(y_pred, dim=1) # torch.max() returns the maximum value and the argmax (index of the maximum value)\n",
        "        train_acc = (y_train == predicted).sum().float() / len(y_train)\n",
        "        losses[\"train\"].append(loss.item() ) # keep track of our training loss\n",
        "\n",
        "        # Run model on validation data\n",
        "        val_loss, val_acc = calculateLossAcc(criterion, model, x_val, y_val) # Call our helper function (see below) on the validation set\n",
        "        losses[\"val\"].append(val_loss.item()) # keep track of our validation loss\n",
        "\n",
        "        # Create plots\n",
        "        display.clear_output(wait=True)\n",
        "        draw_plots(x_val, y_val, model, losses, visualize_surface, visualize_regressor = False)\n",
        "\n",
        "        print(\"Training: [EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), train_acc))        \n",
        "        print(\"Validation: [EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, val_loss.item(), val_acc))\n",
        "        \n",
        "    return losses # In case we want to plot them afterwards\n",
        "\n",
        "# A helper function that calculates our loss and accuracy on a given dataset (by default on our validation set)\n",
        "def calculateLossAcc(criterion, model, x, y):\n",
        "\n",
        "    # set model in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad(): # do not compute gradients for validation\n",
        "        y_pred = model(x)\n",
        "\n",
        "    # compute loss and accuracy \n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "    loss = criterion(y_pred.squeeze(), y)\n",
        "    acc = (y == predicted).sum().float() / len(y)\n",
        "\n",
        "    return loss, acc\n"
      ],
      "metadata": {
        "id": "ms-wGUtsGKCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = train(xManyClassTrain, yManyClassTrain, xManyClassVal, yManyClassVal, criterion, modelMLP3, optimizer, visualize_surface=True, epochs = 100)"
      ],
      "metadata": {
        "id": "Z8YMowM1GKAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bBxD8bYLpwK"
      },
      "outputs": [],
      "source": [
        "del xManyClassTrain, yManyClassTrain, xManyClassVal, yManyClassVal\n",
        "del modelMLP3, optimizer, criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nuD64isLpwK"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}