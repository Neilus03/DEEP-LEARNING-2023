{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will see how to define and use a Convolutional Neural Network (CNN) for clasifying images.\n",
    "\n",
    "Contrary to the MLPs we have used up to know, where each unit connects to all the units of the previous layer, a unit in a convoutional layer only \"sees\" a small region of the layer before, and applies a filter - a local operation - on this small region. The key idea is to apply this same filter to the whole image in a grid fashion, which results into a map of activations for this filter. Using many filters, results into a set of activation maps that when stacked together will form the output of the convolutional layer (each activation map corresponding to a \"channel\").\n",
    "\n",
    "Typically, convolutions are followed by pooling layers that reduce the dimensionality of our intermedate result, and give us every time a little bit more of viewpoint invariance (at the cost of losing precise information about the location of things).\n",
    "\n",
    "When dealing with images, our inputs, and all intermediate tensors we produce through convolution layers will be 3-dimensional tensors. And if we vectorise our operations and pass a whole batch of images in a single go, we will be dealing with 4-dimensional tensors. The ordering of the dimensions in pytorch is $(BatchSize \\times nChannels \\times Height \\times Width)$.\n",
    "\n",
    "After a series of convolutions and pooling operations, we typically want to flaten the images. The flattening operation collapses an array into a 1-dimensional vector. For example, if we have a tensor of $20 \\times 5 \\times 5$ $(C \\times H \\times W)$, its flattened version is a 1d vector of $500$ values. Now we can feed these $500$ values into a MLP for classifiying the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIr2ty0tFA4C"
   },
   "outputs": [],
   "source": [
    "import torch #should be installed by default in any colab notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.numel()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWLIxo9Oigfo"
   },
   "outputs": [],
   "source": [
    "# If this cell fails you need to change the runtime of your colab notebook to GPU\n",
    "# Go to Runtime -> Change Runtime Type and select GPU\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI_YXyigdTUC"
   },
   "source": [
    "# Data\n",
    "\n",
    "Before training we need data! For this notebook we will use the FashionMNIST dataset. Which is available in [torchvision](https://pytorch.org/vision/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7kr-LrSZK7h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = datasets.FashionMNIST(\"data\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.RandomHorizontalFlip()]))\n",
    "val_set = datasets.FashionMNIST(\"data\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=512, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images of FashionMNIST are of size $(28 \\times 28)$ pixels, and have a single channel - the greyscale one. So they are given as a 2D tensor. You can easily check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_set.data[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyEjLgfjrWzs"
   },
   "source": [
    "## Filters and Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a CNN model, similarly to the MLP case, we need to define all the operations and then combine them into the forward pass. Apart from `nn.Linear()` layers, we will now use `nn.Conv2d()` layers, which correspond to 2D convolutions.\n",
    "\n",
    "To perform a convolution, we need to define the size of our filters, and how many filters to use.\n",
    "\n",
    "The size of the filter is given by its width and height (which will be equal), and the number of channels which has to be the same as the number of channels of the input it will be applied to. So, if `in_channels` is the number of channels of the input, and `kernel_size` is the width and height of our filter, then our filter will be a tensor of shape $(in\\_channels \\times kernel\\_size \\times kernel\\_size)$.\n",
    "\n",
    "The other piece of information needed is how many filters to use. Remember that each filter gives rise to one activation map, which in turn corresponds to one channel in the output. So the number of filters to use is the same as the number of output channels (`out_channels`) we want to have.\n",
    "\n",
    "Internally, the set of weights that represent all the filters of a convolutional layer is stored as a single 4D tensor of shape (Number of filters, Number of channels, Height, Width), which would be equivalently $(out\\_channels \\times in\\_channels \\times kernel\\_size \\times kernel\\_size)$\n",
    "\n",
    "This is the minimum information needed to define a convolution layer. Unless otherwise specified, this would apply a default padding of zero and stride of 1. Have a look at the documentation for more details on the `nn.Conv2d()` layer.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv#torch.nn.Conv2d\n",
    "\n",
    "Finally, remember the formula to calculate the size of the output $o$ given the input size $n$ the filter size $f$ the padding $p$ and the stride $s$:\n",
    "\n",
    "$o = {\\frac {(n + 2p - f)} s} + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "**Exercise 1**: Given the following convolutional layer:\n",
    "\n",
    "`nn.Convd2(in_channels=20, out_channels=128, kernel_size=3)`\n",
    "\n",
    "What is the shape of the weights and biases of the convolutional layer?\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Your Answer Here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Convolutional Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define now our first CNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_feature, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, self.n_feature)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Exercise 2**: The model defined above has a shape mismatch problem. Calculate the correct input dimension for `fc1` and change its definition in the `__init__()` method and the call to `x.view()` in the forward pass. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our CNN we need to create our model and define our hyperparameters. We will use an output size of 10, as there are 10 classes in FashionMNIST, and we will train with cross entropy loss and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAoS_kiRZPYT",
    "outputId": "c47f6f6a-4ce9-4e43-f520-898cf464a8fd"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "learning_rate = 1e-2\n",
    "lambda_l2 = 1e-5\n",
    "momentum = 0.5\n",
    "torch.manual_seed(0) # seed for reproductibility\n",
    "\n",
    "output_size = 10  # there are 10 classes\n",
    "\n",
    "model = CNN(6, output_size)\n",
    "\n",
    "print(f\"Number of parameters {get_n_params(model)}:\")\n",
    "\n",
    "# move model to gpu if available\n",
    "model.to(device)\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss() # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "\n",
    "# we use the optim package to apply\n",
    "# stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=lambda_l2) # built-in L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "**Exercise 3**: What does the line `torch.backends.cudnn.benchmark = True` do? How will it affect our runtime if the size of the input images vary a lot?</font>\n",
    "\n",
    "> Hint: Have a look at https://pytorch.org/docs/stable/backends.html\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Your Answer Here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also define our training and validation loops. These are similar to previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # prevent this function from computing gradients \n",
    "def validate(criterion, model, loader):\n",
    "\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for data, target in loader:\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        val_loss += loss.item()                                                              \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss /= len(loader.dataset)\n",
    "    accuracy = 100. * correct / len(loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(loader.dataset), accuracy))\n",
    "\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train(epoch, criterion, model, optimizer, loader):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(loader.dataset),\n",
    "                100. * batch_idx / len(loader), loss.item()))\n",
    "\n",
    "\n",
    "        total_loss += loss.item()  #.item() is very important here? Why?\n",
    "\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Prc0qFO0SR5"
   },
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "psBKsIFAtkXQ",
    "outputId": "1599d175-2eb0-4c4b-ba24-9eaa88bbd343"
   },
   "outputs": [],
   "source": [
    "losses = {\"train\": [], \"val\": []}\n",
    "for epoch in range(10):\n",
    "\n",
    "    train_loss = train(epoch, criterion, model, optimizer, train_loader)\n",
    "    val_loss = validate(criterion, model, val_loader)\n",
    "    losses[\"train\"].append(train_loss)\n",
    "    losses[\"val\"].append(val_loss)\n",
    "    \n",
    "    plt.plot(losses[\"train\"], label=\"training loss\")\n",
    "    plt.plot(losses[\"val\"], label=\"validation loss\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.000001)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be useful to visualize some qualitative examples of classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    data, target = next(iter(val_loader))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    output = model(data)\n",
    "    predictions = np.argmax(output.cpu().numpy(), axis=1).tolist()\n",
    "    true = target.cpu().numpy().tolist()\n",
    "      \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    print(\"Correctly predicted: {}%\".format(100*sum(p == t for p, t in zip(predictions, true))/len(predictions)) )\n",
    "\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        image = data[i,...].cpu().numpy().reshape((28,28))\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "        plt.title('Predicted as {}\\n True label is {}'.format(val_set.classes[predictions[i]], val_set.classes[true[i]], ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "**Exercise 4**: Now repeat the same process, but flip the images horizontally before you run them through the model. Is your accuracy affected? Why?\n",
    "\n",
    "> Hint: Check out the `numpy.flip()` function at https://pytorch.org/docs/stable/generated/torch.flip.html\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that would be interesting to visualise is the weights of the filters. To do so, we just need to access the weights of the convolutional layer of interest, and turn them into images. Here's an example for the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model.conv1.weight.data.cpu().numpy()\n",
    "\n",
    "# Display the filters\n",
    "fig, ax = plt.subplots(1, weight.shape[0], figsize=(5, 5))\n",
    "\n",
    "for i in range(weight.shape[0]):\n",
    "    ax[i].imshow(weight[i, 0], cmap='gray') # show the first (and only) channel weights\n",
    "    ax[i].axes.get_xaxis().set_visible(False)        \n",
    "    ax[i].axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Activation Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing would be to visualise the activations of a convolutional layer for a specific image when it goes through the model. To do so, we need to somehow capture the output of the convolutional layer during the forward pass. This can be done with the function `nn.Conv2d.register_forward_hook()`. This function accepts as argument a callback function that will be called by the model when the forward pass takes place.\n",
    "\n",
    "See some more information about this function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d.register_forward_hook??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try this out, we will defineby hand a toy model that does edge detection, as well as a couple of images to apply it on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEdgeDetector = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "\n",
    "# Define a vertical edge detector filter\n",
    "simpleEdgeDetector.weight.data[0, 0] = torch.tensor([  [1., 0., -1.],\n",
    "                                                   [1., 0., -1.],\n",
    "                                                   [1., 0., -1.]])\n",
    "\n",
    "# Define an horizontal edge detector filter\n",
    "simpleEdgeDetector.weight.data[1, 0] = torch.tensor([  [1., 1., 1.],\n",
    "                                                   [0., 0., 0.],\n",
    "                                                   [-1., -1., -1.]])\n",
    "\n",
    "simpleEdgeDetector.bias.data = torch.zeros(2)\n",
    "\n",
    "simpleEdgeDetector = simpleEdgeDetector.to(device)\n",
    "\n",
    "\n",
    "fakeImageBatch = torch.zeros(2, 1, 26, 26) # create a batch of two black images of (10, 10)\n",
    "fakeImageBatch[0,0,3,3] = 1 # Paint white pixel (3, 3)\n",
    "fakeImageBatch[1,0,6,6] = 1 # Paint white pixel (6, 6)\n",
    "\n",
    "# Show the images\n",
    "fig, ax = plt.subplots(2, 1, figsize=(5, 5))\n",
    "ax[0].imshow(fakeImageBatch[0,0], cmap='gray')\n",
    "ax[1].imshow(fakeImageBatch[1,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define some helper functions to plot a grid of activation maps given the output of a convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(img, ax):\n",
    "    # convert the tensor to numpy\n",
    "    out = img.numpy()\n",
    "    # Bring to the 0-255 range\n",
    "    out = out - out.min()\n",
    "    out = out / out.max()\n",
    "    out = out * 255\n",
    "    out = out.astype('uint8')\n",
    "    # Plot image\n",
    "    ax.axes.get_xaxis().set_visible(False)        \n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.imshow(out, cmap='gray')\n",
    "\n",
    "def showConvMap(conv_map):\n",
    "    # Create a grid of images\n",
    "    h = conv_map.shape[0] # = number of images in the batch\n",
    "    w = conv_map.shape[1] # = number of activation maps per image\n",
    "    fig, ax = plt.subplots(h, w, figsize=(10, 10))\n",
    "\n",
    "    # Plot activation maps\n",
    "    for i in range(conv_map.shape[0]):\n",
    "        for j in range(conv_map.shape[1]):\n",
    "            showImage(conv_map[i][j], ax[i, j])\n",
    "    \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our hook function, which just calls `showConvMap()` to plot the output activation maps for all the images of our batch and all the filters of this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_ShowOutput(module, input, output):\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    showConvMap(output.cpu())\n",
    "\n",
    "fakeImageBatch = fakeImageBatch.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    handle = simpleEdgeDetector.register_forward_hook(hook_ShowOutput)\n",
    "    embed = simpleEdgeDetector(fakeImageBatch)\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "**Exercise 5**: Visualize the feature (activation) maps of the first and last convolutional layers of our CNN model for 10 images of your validation set. Use the code provided before as the basis for your visualisation. What conclusions can you draw?\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErpCTuJ5k4ZH"
   },
   "source": [
    "---\n",
    "\n",
    "*Your Answer Here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP7GGRjqydNDmAjrQ50Ladi",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Intro_to_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
